# Project Title

## Description
This project implements an end-to-end WSI segmentation pipeline.
# Whole-Slide Image Segmentation Pipeline

## Overview
End-to-end pipeline for semantic segmentation of histopathology WSIs using tiling, stain normalization, data augmentation, UNet segmentation, and stitching.

## Directory Structure
```
wsi-segmentation-pipeline/
├── data/
│   ├── raw_slides/           # Place WSI files (.svs, .tif) here
│   ├── processed_tiles/      # Tiles generated by tiling script
│   ├── masks/                # Ground-truth masks for training
│   ├── reference_tile.png    # Reference tile for normalization
├── notebooks/
│   └── wsi_pipeline_demo.ipynb
├── src/
│   ├── tiling.py
│   ├── stain_normalization.py
│   ├── augmentation.py
│   ├── unet_model.py
│   ├── train.py
│   ├── inference.py
│   ├── stitching.py
│   └── utils.py
├── requirements.txt
├── README.md
└── config.yaml (optional)
```

## Setup
1. **Install dependencies:**
    ```
    pip install -r requirements.txt
    ```
2. **Prepare your data:**
    - Download WSIs and masks from [CAMELYON16](https://camelyon16.grand-challenge.org/Data/), [MoNuSeg](https://monuseg.grand-challenge.org/Data/), or [PanNuke](https://warwick.ac.uk/fac/sci/dcs/research/tia/pannuke).
    - Place files in the appropriate folders in `data/`.

## Pipeline Steps

1. **Tiling:**
    ```python
    from src.tiling import tile_wsi
    tile_wsi('data/raw_slides/sample.svs', 'data/processed_tiles/', tile_size=1024, overlap=0)
    ```
2. **Stain Normalization:**
    ```python
    from src.stain_normalization import normalize_tiles
    normalize_tiles('data/processed_tiles/', 'data/reference_tile.png', 'data/norm_tiles/')
    ```
3. **Augmentation:**
    ```python
    from src.augmentation import augment_tiles
    augment_tiles('data/norm_tiles/', 'data/aug_tiles/', num_aug=5)
    ```
4. **Training:**
    ```python
    from src.train import train
    model = train('data/aug_tiles/', 'data/masks/', epochs=10, batch_size=4, lr=1e-4, device='cuda')
    ```
5. **Inference:**
    ```python
    from src.inference import run_inference
    run_inference('data/aug_tiles/', 'checkpoint_epoch10.pt', 'data/pred_masks/', device='cuda')
    ```
6. **Stitching:**
    ```python
    from src.stitching import stitch_masks
    stitch_masks('data/pred_masks/', 'data/full_mask.png', (50000, 50000), tile_size=1024, overlap=0)
    ```

## Example Data Links

- [CAMELYON16](https://camelyon16.grand-challenge.org/Data/) (WSI + masks)
- [MoNuSeg](https://monuseg.grand-challenge.org/Data/) (tiles + masks)
- [PanNuke](https://warwick.ac.uk/fac/sci/dcs/research/tia/pannuke) (tiles + masks)
- [TCGA Portal](https://portal.gdc.cancer.gov/) (WSI, registration required)

## Notes
- Adjust parameters and file paths as needed.
- For full WSIs, tiling is required. For tile-based datasets like PanNuke, you can skip tiling.
- Scripts are modular; you can run each step independently.

## License
MIT
